

---

Step 1: Package Structure and Core Code with Feature Engineering

Here’s a detailed modular structure:

azimuth_predictor/
├── azimuth_predictor/
│   ├── __init__.py
│   ├── features.py
│   ├── model.py
│   ├── api.py
│   ├── utils.py
├── data/
│   └── example_training_data.csv
├── tests/
│   ├── test_features.py
│   ├── test_model.py
├── Dockerfile
├── requirements.txt
├── setup.py
└── README.md


---

1.1 features.py — Enhanced Feature Engineering

import numpy as np

class CRISPRFeatureExtractor:
    nucleotide_alphabet = ['A', 'C', 'G', 'T']

    def encode_nucleotide_identity(self, seq):
        # One-hot encode nucleotides
        encoding = []
        for nt in seq.upper():
            encoding.extend([int(nt == base) for base in self.nucleotide_alphabet])
        return np.array(encoding)

    def calculate_gc_content(self, seq):
        gc_count = seq.count('G') + seq.count('C')
        return gc_count / len(seq) if len(seq) > 0 else 0

    def calculate_melting_temp(self, seq):
        # Basic Wallace rule: 2°C per A/T and 4°C per G/C
        at = seq.count('A') + seq.count('T')
        gc = seq.count('G') + seq.count('C')
        return 2 * at + 4 * gc

    def thermodynamic_properties(self, seq):
        gc_content = self.calculate_gc_content(seq)
        melting_temp = self.calculate_melting_temp(seq)
        return np.array([gc_content, melting_temp])

    def microhomology_features(self, seq, window=5):
        """
        Simplified microhomology score: count repeats within window near cut site.
        """
        score = 0
        for i in range(len(seq) - window + 1):
            kmer = seq[i:i+window]
            count = seq.count(kmer)
            if count > 1:
                score += count - 1
        return np.array([score])

    def extract_features(self, seq):
        return np.concatenate([
            self.encode_nucleotide_identity(seq),
            self.thermodynamic_properties(seq),
            self.microhomology_features(seq)
        ])


---

1.2 model.py — Training, Prediction & Evaluation

import numpy as np
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import r2_score, mean_squared_error
from .features import CRISPRFeatureExtractor

class AzimuthModel:
    def __init__(self):
        self.model = GradientBoostingRegressor(n_estimators=150, max_depth=5, random_state=42)
        self.feature_extractor = CRISPRFeatureExtractor()
        self.is_trained = False

    def train(self, sequences, efficiencies):
        X = np.array([self.feature_extractor.extract_features(seq) for seq in sequences])
        y = np.array(efficiencies)
        self.model.fit(X, y)
        self.is_trained = True

    def predict(self, sequences):
        if not self.is_trained:
            raise RuntimeError("Model not trained.")
        X = np.array([self.feature_extractor.extract_features(seq) for seq in sequences])
        preds = self.model.predict(X)
        return np.clip(preds, 0, 1)

    def evaluate(self, sequences, true_efficiencies):
        preds = self.predict(sequences)
        r2 = r2_score(true_efficiencies, preds)
        rmse = mean_squared_error(true_efficiencies, preds, squared=False)
        return {"r2": r2, "rmse": rmse}


---

1.3 api.py — Flask REST API Service

from flask import Flask, request, jsonify
from .model import AzimuthModel

app = Flask(__name__)
model = AzimuthModel()

@app.route('/train', methods=['POST'])
def train():
    data = request.get_json()
    sequences = data.get('sequences', [])
    efficiencies = data.get('efficiencies', [])
    if not sequences or not efficiencies:
        return jsonify({"error": "Sequences and efficiencies required"}), 400
    model.train(sequences, efficiencies)
    return jsonify({"message": f"Model trained on {len(sequences)} samples."})

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json()
    sequences = data.get('sequences', [])
    if not sequences:
        return jsonify({"error": "Sequences required"}), 400
    try:
        predictions = model.predict(sequences).tolist()
        return jsonify({"predictions": predictions})
    except RuntimeError as e:
        return jsonify({"error": str(e)}), 400

@app.route('/evaluate', methods=['POST'])
def evaluate():
    data = request.get_json()
    sequences = data.get('sequences', [])
    true_efficiencies = data.get('true_efficiencies', [])
    if not sequences or not true_efficiencies:
        return jsonify({"error": "Sequences and true_efficiencies required"}), 400
    metrics = model.evaluate(sequences, true_efficiencies)
    return jsonify(metrics)

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)


---

1.4 setup.py and requirements.txt

setup.py (simplified):

from setuptools import setup, find_packages

setup(
    name='azimuth_predictor',
    version='0.1.0',
    packages=find_packages(),
    install_requires=[
        'numpy>=1.21',
        'scikit-learn>=1.0',
        'flask>=2.0'
    ],
    entry_points={
        'console_scripts': [
            'azimuth-api=azimuth_predictor.api:app.run',
        ],
    },
)

requirements.txt:

numpy>=1.21
scikit-learn>=1.0
flask>=2.0


---

Step 2: Dockerize the REST API Service

Dockerfile:

FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY azimuth_predictor ./azimuth_predictor

ENV FLASK_APP=azimuth_predictor.api
ENV FLASK_RUN_HOST=0.0.0.0
ENV FLASK_RUN_PORT=5000

EXPOSE 5000

CMD ["flask", "run"]


---

Step 3: Integration with Experimental Data Pipelines & Frontend Scaffold

Data pipelines:

Save CSV/JSON training data in /data

Scripts for batch training and evaluation can be added under /scripts (optional)


Frontend (example):

You could use a simple React or Vue frontend to call /predict API to get guide RNA efficiencies and visualize them with charts. This scaffold can be added on demand.



---

Step 4: Testing

Sample test snippet for feature extraction (tests/test_features.py):

from azimuth_predictor.features import CRISPRFeatureExtractor

def test_feature_extraction():
    extractor = CRISPRFeatureExtractor()
    seq = "ACGT"
    features = extractor.extract_features(seq)
    assert len(features) == (4 * 4) + 2 + 1  # 4 nucleotides * 4 + thermodynamic(2) + microhomology(1)


---

Summary and Next Steps

I prepared the full modular package for Azimuth-style CRISPR guide RNA efficiency prediction.

You can build and deploy the REST API quickly with Docker.

The feature engineering is realistic for nucleotide sequences, thermodynamics, and microhomology.

Integration with data pipelines and frontends can be done modularly.

Tests are included for robust development.



---